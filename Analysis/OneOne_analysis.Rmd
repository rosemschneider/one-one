---
title: "OneOne_analysis"
author: "Rose M. Schneider"
date: "4/10/2018"
output: html_document
---
###Setup
```{r, include = FALSE}
rm(list = ls())
library(tidyverse)
library(magrittr)
library(langcog)
library(lme4)
library(stringr)
library(RColorBrewer)
library(ggthemes)

#filtering function
'%!in%' <- function(x,y)!('%in%'(x,y))

#set root
require("knitr")
# knitr::opts_knit$set(root.dir = fs::path(here::here())) #this is specific to RMS, change accordingly
```

###Load Data
```{r}
data.raw <- read.csv("../Data/one-one_data.csv")%>%
  filter(SID != "CopyPasteMe")%>%
  droplevels()%>%
  mutate(Age = as.numeric(as.character(Age)), 
         Agegroup = cut(Age, breaks = c(3, 3.5, 4, 4.5, 5.1)))%>%
  mutate(CP_subset = ifelse(Knower_level == "CP", "CP", "Subset"), 
          CP_subset = factor(CP_subset))
```

###Exclusions
How many children were tested, pre-exclusions?
```{r}
#how many kids pre-exclusion
data.raw %>% 
  distinct(SID, CP_subset)%>%
  group_by(CP_subset)%>%
  summarise(n = n())%>%
  mutate(total.n = sum(n))
```

Why are children excluded?
```{r}
#why are kids excluded?
data.raw %>%
  filter(Exclude == 1)%>%
  distinct(SID, Exclude_reason)%>%
  group_by(Exclude_reason)%>%
  summarise(n = n())
```

Exclude these children from the analysis
```{r}
##exclude these kids from analysis
all.data <- data.raw %<>%
  filter(Exclude != 1)
```

###Post-hoc exclusion
Determining whether children did not understand the task; using failure on both Parallel training trials as a diagnostic. If children really do not understand the task, they should fail both trials of the parallel condition. This is looking at children who fail at least one trial in the Parallel condition to determine whether they do not understand the task. 
```{r}
#how many kids failed at least one trial in Parallel
failed.trials <- all.data %>%
  filter(Task == "Parallel" | Task == "Orthogonal",
         Trial_number == "Training")%>%
  distinct(SID, CP_subset, Task, Task_item, Correct)%>%
  filter(Correct == "0")

#first look at the kids who failed both on parallel, as these are likely to not understand the task 
failed.trials %>%
  filter(Task == "Parallel")%>%
  group_by(SID, CP_subset)%>%
  summarise(n = n())%>%
  filter(n == 2)

#021219-KT - gives max for every single trial, failed training trials even with feedback, should be excluded 
#021919-EL - should not be excluded - failed both Parallel training (15 for both), but seemed to get it (matched for 3 and 4)
#022519-ER - looked a little confused on training trials at the start, but seemed to get it
#022619-JM - marginal, failed both parallel (15 for both), some variability after that, succeeded on orthogonal training, but failed on 3 and 4; should not be excluded, succeeded on Orthogonal training
#022719-SW - marginal, seems to be performing randomly, succeeded on only one training trial
#022819-BT - should not be excluded, failed on first two parallel, but then seemed to recover, succeeds on orthogonal training

##MANUAL EXCLUSION for complete failure on set-matching
all.data %<>%
  filter(SID != '021219-KT')
```

##Demographics
Ages and ns by knower level
```{r}
all.data %>%
  distinct(SID, Age, Knower_level)%>%
  group_by(Knower_level)%>%
  summarise(n = n(), 
            mean_age = mean(Age, na.rm = TRUE), 
            sd_age = sd(Age, na.rm = TRUE))%>%
  kable()
```

Ages and ns by CP/Subset-knower status
```{r}
all.data %>%
  distinct(SID, CP_subset, Age)%>%
  group_by(CP_subset)%>%
  summarise(n = n(), 
            mean_age = mean(Age, na.rm = TRUE), 
            sd_age = sd(Age, na.rm = TRUE))%>%
  kable()
```

Overall age and sex information
```{r}
#overall mean age
all.data %>%
  distinct(SID, Age)%>%
  summarise(mean_age = mean(Age, na.rm = TRUE), 
            sd_age = sd(Age, na.rm = TRUE))

#sex information
all.data %>%
  distinct(SID, Sex)%>%
  group_by(Sex)%>%
  summarise(n = n())
```



---

##Data manipulations 
Get counting proficiency for each participant: 
Foor each set (8/10), participants received a score of 3 (perfect); 2 (counted incorrectly but fixed mistake); 1 (counted only a few correctly); 0 (counted randomly)

```{r}
#fix the name of the task for kids who weren't run on two trials of 10
tmp <- all.data %>%
  filter(Task == "How Many")%>%
  mutate(Task_item = ifelse(Task_item == "Score", "10 - Score", as.character(Task_item)))

#compute mean counting
ms.count <- tmp %>%
  filter(Task_item == "10 - Score" | 
           Task_item == "8 - Score")%>%
  distinct(SID, Task_item, Response)%>%
  group_by(SID)%>%
  summarise(count_proficiency = mean(as.numeric(as.character(Response, na.rm = TRUE))))%>%
  dplyr::select(SID, count_proficiency)

##add to all.data
all.data <- right_join(all.data, ms.count, by = "SID")%>%
  dplyr::select(-X, - X.1, -X.2, -X.3, -X.4)
```

####Get highest count for each participant
```{r}
hc.lookup <- all.data %>%
  filter(Task_item == "Highest_count")%>%
  dplyr::select(SID, Response)%>%
  dplyr::rename(highest_count = Response)

##There are several children who will not count out loud, exclude them from analyses with Highest Count

all.data <- right_join(all.data, hc.lookup, by = "SID")
```


###Counting attempts on Give-N by knower level 
```{r}
# given.ms <- all.data %>%
#   filter(Task == "Give_N", 
#          !is.na(Task_item))%>%
#   group_by(Knower_level, Task_item)%>%
#   summarise(num_counting = sum(as.numeric(as.character(Counting.Number.language))))
```

---

#Set-matching tasks: Visualizations
##Mean performance by group
```{r}
#add numerosity classification for easier-to-read graphs
all.data %<>%
  mutate(Numerosity = ifelse((Task == "Parallel" & as.numeric(as.character(Task_item)) < 5),
                              "Small", 
                              ifelse((Task == "Parallel" & as.numeric(as.character(Task_item)) >5), "Large", 
                                     ifelse((Task == "Orthogonal" & as.numeric(as.character(Task_item)) < 5), "Small", 
                                            ifelse((Task == "Orthogonal" & as.numeric(as.character(Task_item)) > 5), "Large", "NA")))))

all.data %>%
  filter(Task == "Parallel" | Task == "Orthogonal", 
         Trial_number != "Training")%>%
  mutate(Task_item = factor(Task_item, levels = c("3", "4",  
                                                  "6", "8", "10")), 
         Correct = as.numeric(as.character(Correct)))%>%
  group_by(Numerosity, Task, Task_item, CP_subset)%>%
    multi_boot_standard("Correct", na.rm = TRUE)%>%
  ggplot(aes(x = Task_item, y = mean, colour = CP_subset, group = interaction(Numerosity, CP_subset))) +
  geom_point(size = 2) + 
  geom_line() +
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper), 
                width = .1) +
  theme_bw(base_size = 15) + 
  facet_grid(~factor(Task, levels = c("Parallel", "Orthogonal")), scale = "free_x") +
  theme( 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), 
        legend.title = element_blank(), 
        legend.position = "right") +
  labs(x = "Set size", y = "Mean accuracy") +
  scale_colour_brewer(palette = "Dark2") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  labs(color= "Knower Level")

ggsave("/Users/roseschneider/Documents/Projects/one-one/Analysis/Figures/mean_accuracy.png", width = 5.8, height = 3.5)
```

##Visualization by condition (Identical/Non-Identical)
```{r}
all.data %>%
  filter(Task == "Parallel" | Task == "Orthogonal", 
         Trial_number != "Training")%>%
  mutate(Task_item = factor(Task_item, levels = c("3", "4", 
                                                  "6", "8", "10")), 
         Condition = factor(Condition, levels = c("IDENTICAL", "NONIDENTICAL"), 
                            labels = c("Identical", "Nonidentical")), 
         Correct = as.numeric(as.character(Correct)))%>%
  group_by(Numerosity, Task, Task_item, Condition, CP_subset)%>%
  langcog::multi_boot_standard("Correct", na.rm = TRUE)%>%
  ggplot(aes(x = Task_item, y = mean, colour = CP_subset, group = interaction(Numerosity, CP_subset))) +
  geom_point(size = 2) + 
  geom_line() +
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper), 
                width = .1) +
  theme_bw(base_size = 15) + 
  facet_grid(factor(Task, levels = c("Parallel", "Orthogonal"))~Condition, scale = "free_x") +
  theme(legend.position = "right", 
        panel.grid = element_blank(), 
        legend.title = element_blank()) +
  labs(x = "Set size", y = "Mean performance") +
  scale_colour_brewer(palette = "Dark2") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  + 
  labs(color = "Knower Level")

ggsave("/Users/roseschneider/Documents/Projects/one-one/Analysis/Figures/mean_accuracy_condition.png", width = 6, height = 4.2)
```

##Distribution of responses for parallel set: Overall, CP vs. Subset 
Looking at variance around the requested item; this is for both identical and non-identical
```{r}
all.data %>%
  filter(Task == "Parallel")%>%
  mutate(Task_item = as.numeric(as.character(Task_item)), 
         Response = as.numeric(as.character(Response)))%>%
  filter(Task_item > 2)%>%
  group_by(CP_subset, Task_item, Response)%>%
  # summarise(n = n()) %>%
  ggplot(aes(x = Response, fill = CP_subset)) +
  geom_vline(aes(xintercept = Task_item), linetype = "dashed") +
  geom_histogram() + 
  theme_bw(base_size = 9) +
  theme(legend.position = "none", 
        axis.text.x = element_text(angle = 45, hjust = 1)) + 
  scale_fill_brewer(palette = "Dark2") +
  facet_grid(CP_subset~Task_item) +
  scale_x_continuous(breaks = seq(1, 15, 1)) + 
  labs(x = 'Number of items given', y = 'Frequency')


ggsave("/Users/roseschneider/Documents/Projects/one-one/Analysis/Figures/dist_response_parallel.png", width = 7, height = 3.5)

```

##Distribution of responses for orthogonal set: Overall, CP vs. Subset 
Looking at variance around the requested item; this is for both identical and non-identical
```{r}
all.data %>%
  filter(Task == "Orthogonal")%>%
  mutate(Task_item = as.numeric(as.character(Task_item)), 
         Response = as.numeric(as.character(Response)))%>%
  filter(Task_item > 2)%>%
  group_by(CP_subset, Task_item, Response)%>%
  # summarise(n = n()) %>%
  ggplot(aes(x = Response, fill = CP_subset)) +
  geom_vline(aes(xintercept = Task_item), linetype = "dashed") +
  geom_histogram() + 
  theme_bw(base_size = 9) +
  theme(legend.position = "none", 
        axis.text.x = element_text(angle = 45, hjust = 1)) + 
  scale_fill_brewer(palette = "Dark2") +
  facet_grid(CP_subset~Task_item) +
  scale_x_continuous(breaks = seq(1, 15, 1)) + 
  labs(x = 'Number of items given', y = 'Frequency')


ggsave("/Users/roseschneider/Documents/Projects/one-one/Analysis/Figures/dist_response_orthogonal.png", width = 7, height = 3.5)

```

##Distribution around correct Response: CP-knowers, identical vs. non-identical, Parallel
Looking at variance around the requested item; this is for both identical and non-identical
```{r}
all.data %>%
  filter(Task == "Parallel", 
         CP_subset == "CP")%>%
  mutate(Task_item = as.numeric(as.character(Task_item)), 
         Response = as.numeric(as.character(Response)))%>%
  filter(Task_item > 2)%>%
  group_by(Condition, Task_item, Response)%>%
  # summarise(n = n()) %>%
  ggplot(aes(x = Response, fill = CP_subset)) +
  geom_vline(aes(xintercept = Task_item), linetype = "dashed") +
  geom_histogram() + 
  theme_bw(base_size = 9) +
  theme(legend.position = "none", 
        axis.text.x = element_text(angle = 45, hjust = 1)) + 
  scale_fill_brewer(palette = "Dark2") +
  facet_grid(Condition~Task_item) +
  scale_x_continuous(breaks = seq(1, 15, 1)) + 
  labs(x = 'Number of items given', y = 'Frequency')

```

##Distribution around correct Response: Subset-knowers, identical vs. non-identical, Parallel
Looking at variance around the requested item; this is for both identical and non-identical
```{r}
all.data %>%
  filter(Task == "Parallel", 
         CP_subset == "Subset")%>%
  mutate(Task_item = as.numeric(as.character(Task_item)), 
         Response = as.numeric(as.character(Response)))%>%
  filter(Task_item > 2)%>%
  group_by(Condition, Task_item, Response)%>%
  # summarise(n = n()) %>%
  ggplot(aes(x = Response, fill = CP_subset)) +
  geom_vline(aes(xintercept = Task_item), linetype = "dashed") +
  geom_histogram() + 
  theme_bw(base_size = 9) +
  theme(legend.position = "none", 
        axis.text.x = element_text(angle = 45, hjust = 1)) + 
  scale_fill_manual(values = "#D95F02") +
  facet_grid(Condition~Task_item) +
  scale_x_continuous(breaks = seq(1, 15, 1)) + 
  labs(x = 'Number of items given', y = 'Frequency')

```


##Visualizations of mean absolute error by knower-level for each task 
Make a dataframe containing only the incorrect responses
```{r}
error.df <- all.data %>%
  filter(Task == "Parallel" | Task == "Orthogonal", 
         Trial_number != "Training", 
         Correct == "0")%>%
  droplevels()%>%
  mutate(Task_item = as.numeric(as.character(Task_item)), 
         Response = as.numeric(as.character(Response)), 
         abs_error = abs(Task_item-Response))
```


```{r}
error.df %>%
  mutate(Task_item = factor(Task_item, levels = c("3", "4", 
                                                  "6", "8", "10")))%>%
  group_by(Numerosity, Task, Task_item, CP_subset)%>%
  multi_boot_standard("abs_error", na.rm = TRUE)%>%
  ggplot(aes(x = Task_item, y = mean, colour = CP_subset, group = interaction(Numerosity, CP_subset))) +
  geom_point(size = 2) + 
  geom_line() +
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper), 
                width = .1) +
  theme_bw(base_size = 15) + 
  facet_grid(~factor(Task, levels = c("Parallel", "Orthogonal")), scale = "free_x") +
  theme(legend.position = "right", 
        panel.grid = element_blank(), 
        legend.title = element_blank()) +
  labs(x = "Set size", y = "Mean absolute error") +
  scale_colour_brewer(palette = "Dark2") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  + 
  labs(color = "Knower Level")

ggsave("/Users/roseschneider/Documents/Projects/one-one/Analysis/Figures/mean_abs_error.png", width = 5.8, height = 3.5)
```

##Visualization of mean absolute error by condition
```{r}
error.df %<>%
  mutate(Condition = factor(Condition, levels = c("IDENTICAL", "NONIDENTICAL"), 
                            labels = c("Identical", "Nonidentical")))

error.df %>%
  filter(Correct == "0")%>%
  mutate(Task_item = factor(Task_item, levels = c("3", "4", "5", 
                                                  "6", "7", "8", "9", "10")))%>%
  group_by(Numerosity, Task, Task_item, CP_subset, Condition)%>%
  multi_boot_standard("abs_error", na.rm = TRUE)%>%
  ggplot(aes(x = Task_item, y = mean, colour = CP_subset, group = interaction(Numerosity, CP_subset))) +
  geom_point(size = 2) + 
  geom_line() +
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper), 
                width = .1) +
  theme_bw(base_size = 15) + 
  facet_grid(factor(Task, levels = c("Parallel", "Orthogonal")) ~ Condition, scale = "free_x") +
  theme(legend.position = "right", 
        panel.grid = element_blank(), 
        legend.title = element_blank()) +
  labs(x = "Set size", y = "Mean absolute error") +
  scale_colour_brewer(palette = "Dark2") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  + 
  labs(color = "Knower Level")

ggsave("/Users/roseschneider/Documents/Projects/one-one/Analysis/Figures/mean_abs_error_condition.png", width = 6, height = 4.2)
```

##Counting proficiency
Scatterplot of age and counting proficiency
```{r}
all.data %>%
  filter(!is.na(count_proficiency), 
         count_proficiency != 5) %>% #temporary, coding issue
  distinct(SID, CP_subset, Age, count_proficiency)%>%
  mutate(count_proficiency = factor(count_proficiency))%>%
  ggplot(aes(x = count_proficiency, y = Age, color = count_proficiency, gorup = count_proficiency)) + 
  geom_point() + 
  theme_bw() + 
  facet_grid(~CP_subset)
```

Barplot by knower-level
```{r}
all.data %>%
  filter(!is.na(count_proficiency), 
         count_proficiency != 5) %>% #temporary, coding issue
  distinct(SID, CP_subset, Age, count_proficiency)%>%
  mutate(count_proficiency = factor(count_proficiency))%>%
  ggplot(aes(x = CP_subset, fill = count_proficiency)) + 
  geom_histogram(stat = "count") + 
  theme_bw()
```

##Highest count
```{r}
all.data %>%
  filter(!is.na(highest_count))%>%
  mutate(highest_count = as.numeric(as.character(highest_count)))%>%
  distinct(SID, Age, CP_subset, highest_count)%>%
  ggplot(aes(x = highest_count, fill = CP_subset)) + 
   geom_histogram(color = "black", stat = "count")+
  theme_bw() + 
  scale_fill_brewer(palette = "Set1") + 
  labs(x = "Highest Count (unprompted)", y = "Count")
```

#Scatterplots of mean performance - when are children reaching ceiling on this task?
```{r}
set.ms <- all.data %>%
  filter(!is.na(highest_count), 
         count_proficiency != 5)%>%
  mutate(highest_count = as.numeric(as.character(highest_count)))%>%
  filter(Task == "Parallel" | Task == "Orthogonal")%>%
  group_by(SID, Task, Age, highest_count, count_proficiency, CP_subset)%>%
  summarise(mean = mean(as.numeric(as.character(Correct)), na.rm = TRUE))

##correlation between age and performance
set.ms %>%
  ggplot(aes(x = Age, y = mean, color = CP_subset)) + 
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) + 
  theme_bw() +
  facet_grid(~factor(Task, levels = c("Parallel", "Orthogonal"))) + 
  scale_color_brewer(palette = "Dark2") + 
  labs(title = "Relationship between age and mean performance")

##highest count and mean performance
set.ms %>%
 ggplot(aes(x = highest_count, y = mean, color = CP_subset)) + 
  geom_point(position = position_jitter()) +
  geom_smooth(method = "lm", se = FALSE) + 
  theme_bw(base_size = 15) +
  facet_grid(~factor(Task, levels = c("Parallel", "Orthogonal"))) + 
  theme(panel.grid = element_blank(), 
        legend.position = "top", 
        legend.title = element_blank()) +
  scale_color_brewer(palette = "Dark2") + 
  labs(x = "Highest count", y = "Mean set-matching accuracy", color = "Knower level")

ggsave("/Users/roseschneider/Documents/Projects/one-one/Analysis/Figures/hc_acc.png", width = 5.8, height = 4.2)

##count proficiency and mean performance
set.ms %>%
 ggplot(aes(x = count_proficiency, y = mean, color = CP_subset)) + 
  geom_point(position = position_jitter()) +
  geom_smooth(method = "lm", se = FALSE) + 
  theme_bw(base_size = 15) +
  facet_grid(~factor(Task, levels = c("Parallel", "Orthogonal"))) + 
  scale_color_brewer(palette = "Dark2") + 
  theme(panel.grid = element_blank(), 
        legend.position = "top", 
        legend.title = element_blank()) +
  labs(x = "Counting proficiency", y = "Mean set-matching accuracy", color = "Knower level")

ggsave("/Users/roseschneider/Documents/Projects/one-one/Analysis/Figures/counting_acc.png", width = 5.8, height = 4.2)
```

---

#Analyses
Make accuracy df
```{r}
model.df <- all.data %>%
  filter(Task == "Parallel" | Task == "Orthogonal", 
         Trial_number != "Training")%>%
  mutate(Task_item = as.numeric(as.character(Task_item)), 
         Response = as.numeric(as.character(Response)), 
         abs_error = abs(Task_item - Response), 
         highest_count = as.numeric(as.character(highest_count)), 
         age.c = as.vector(scale(Age, center = TRUE, scale=TRUE)), 
         Task_item.c = as.vector(scale(Task_item, center = TRUE, scale=TRUE)), 
         abs_error.c = as.vector(scale(abs_error, center = TRUE, scale=TRUE)), 
         highest_count.c = as.vector(scale(highest_count, center = TRUE, scale = TRUE)),
         count_proficiency.c = as.vector(scale(count_proficiency, center = TRUE, scale = TRUE)),
         Correct = as.numeric(as.character(Correct)), 
         CP_subset = factor(CP_subset, levels = c("Subset", "CP")))
```
##Do CP-knowers have higher accuracy overall?
Yes - CP-knowers have significantly better performance compared to subset-knoowers, but they do not have a specific benefit for larger numerosities (i.e., no interaction between knower-level and larger numerosities). Worse performance for larger numerosities; better performance for parallel condition. Significantly better performance with age.  
```{r}
#create a base model that includes numerosity and task
overall.acc.base <- glmer(Correct ~ Task_item.c + Task + age.c + (1|SID), 
                          family = "binomial", data = model.df, 
                          control=glmerControl(optimizer="bobyqa",
                          optCtrl=list(maxfun=2e4)))

#add CP_subset-knower status
overall.acc.kl <- glmer(Correct ~ CP_subset + Task_item.c + Task + age.c + (1|SID), 
                          family = "binomial", data = model.df, 
                          control=glmerControl(optimizer="bobyqa",
                          optCtrl=list(maxfun=2e4)))

#now add interaction
overall.acc.kl.int <- glmer(Correct ~ CP_subset*Task_item.c + Task + age.c + (1|SID), 
                          family = "binomial", data = model.df, 
                          control=glmerControl(optimizer="bobyqa",
                          optCtrl=list(maxfun=2e4)))

#compare
anova(overall.acc.base, overall.acc.kl, overall.acc.kl.int, test = 'LRT') 

summary(overall.acc.kl)
```

##Followup analysis: Are CP-knowers significantly more accurate than subset-knowers on Orthogonal task?
Yes; CP-knowers have significantly more accurate performance on the Orthogonal version in comparison to subset knowers.
```{r}
#build the base model
orth.base <- glmer(Correct ~ Task_item.c + age.c + (1|SID), 
                  family= "binomial", data = subset(model.df, Task == "Orthogonal"), 
                  control=glmerControl(optimizer="bobyqa",
                          optCtrl=list(maxfun=2e4)))

#now add KL
orth.kl <- glmer(Correct ~ CP_subset+ Task_item.c + age.c + (1|SID), 
                  family= "binomial", data = subset(model.df, Task == "Orthogonal"), 
                  control=glmerControl(optimizer="bobyqa",
                          optCtrl=list(maxfun=2e4)))

#what about an interaction
orth.kl.int <- glmer(Correct ~ CP_subset*Task_item.c + age.c + (1|SID), 
                  family= "binomial", data = subset(model.df, Task == "Orthogonal"), 
                  control=glmerControl(optimizer="bobyqa",
                          optCtrl=list(maxfun=2e4)))


#does interaction significantly improve fit of the model?
anova(orth.base, orth.kl, orth.kl.int, test = 'LRT')
summary(orth.kl.int)
```

##To-do: follow-up analyseis on specific numerosities (6, 8, 10)

##Analysis: Does identity of items matter? 
Yes; There is a main effect of identity, with worse performance overall for nonidentical items. Effect of CP-status is marginal (*p* = .08); Significantly worse performance for larger set sizes; Better performance for parallel condition. 

Interaction: CP-knowers have significantly better performance on NONIDENTICAL condition in comparison to subset-knowers.
```{r}
#make base model without condition term 
condition.acc.base <- glmer(Correct ~ Condition + CP_subset + Task_item.c + Task + age.c + (1|SID), 
                            family = "binomial", data = model.df)

##now add interaction - 2-way
condition.acc.2int <- glmer(Correct ~ Condition*CP_subset + Task_item.c + Task + age.c + (1|SID), 
                            family = "binomial", data = model.df, 
                            control=glmerControl(optimizer="bobyqa",
                          optCtrl=list(maxfun=2e4)))

##now add 3-way interaction
condition.acc.3int <- glmer(Correct ~ Condition*CP_subset*Task_item.c + Task + age.c + (1|SID), 
                            family = "binomial", data = model.df, 
                            control=glmerControl(optimizer="bobyqa",
                          optCtrl=list(maxfun=2e4)))

#compare
anova(condition.acc.base, condition.acc.2int, condition.acc.3int, test = 'LRT')

##There is a 2-way interaction between CP_subset and condition, such that subset-knowers are significantly worse on non-identical condition
summary(condition.acc.2int)
```


####Are SS-knowers more accurate for identical in comparison to nonidentical?
```{r}
ss.ident.base <- glmer(Correct ~ Task_item.c + Task + age.c + (1|SID), 
                  family = "binomial", data = subset(model.df, CP_subset == "Subset"))

#add condition
ss.ident.condition <- glmer(Correct ~ Condition + Task_item.c + Task + age.c + (1|SID), 
                  family = "binomial", data = subset(model.df, CP_subset == "Subset"))

#add interaction
ss.ident.condition.int <- glmer(Correct ~ Condition*Task_item.c + Task + age.c + (1|SID), 
                  family = "binomial", data = subset(model.df, CP_subset == "Subset"))

#compare
anova(ss.ident.base, ss.ident.condition, ss.ident.condition.int, test = 'LRT')

#main effect of condition
summary(ss.ident.condition)
```

####Are CP-knowers more accurate for non-identical in comparison to identical?
```{r}
cp.ident.base <- glmer(Correct ~ Task_item.c + Task + age.c + (1|SID), 
                  family = "binomial", data = subset(model.df, CP_subset == "CP"))

#add condition
cp.ident.condition <- glmer(Correct ~ Condition + Task_item.c + Task + age.c + (1|SID), 
                  family = "binomial", data = subset(model.df, CP_subset == "CP"))

#add interaction
cp.ident.condition.int <- glmer(Correct ~ Condition*Task_item.c + Task + age.c + (1|SID), 
                  family = "binomial", data = subset(model.df, CP_subset == "CP"))

#compare
anova(cp.ident.base, cp.ident.condition, cp.ident.condition.int, test = 'LRT')

#marginal effect of condition
summary(cp.ident.condition) ##very slightly better performance for 
```

##To-do: follow up analyses with numerosity

##Error: Do CP_knowers have lower rates of absolute error?
Yes; On trials with incorrect responses, CP-knowers have lower mean absolute error overall. 

```{r}
error.model.df <- model.df %>%
  filter(Correct == 0)

#base model
overall.error.base <- lmer(abs_error ~ Task_item.c + Task + age.c + (1|SID), 
                           data = error.model.df)

#add kl
overall.error.kl <- lmer(abs_error ~ CP_subset + Task_item.c + Task + age.c + (1|SID), 
                           data = error.model.df)

#add interaction
overall.error.int <- lmer(abs_error ~ CP_subset*Task_item.c + Task + age.c + (1|SID), 
                           data = error.model.df)

#compare 
anova(overall.error.base, overall.error.kl, overall.error.int, test = 'LRT') #subset-knowers have higher absolute error for greater set sizes; significantly lower error on parallel task
summary(overall.error.kl)
```

##Follow-up: Do CP-knowers have lower absolute error for orthogonal condition? 
Yes, overall absolute error, but no specific benefit for larger numerosities
```{r}
#base model
orth.overall.error.base <- lmer(abs_error ~ Task_item.c + age.c + (1|SID), 
                           data = subset(error.model.df, Task == "Orthogonal"))

#add kl
orth.overall.error.kl <- lmer(abs_error ~ CP_subset + Task_item.c + age.c + (1|SID), 
                           data = subset(error.model.df, Task == "Orthogonal"))

#add interaction
orth.overall.error.int <- lmer(abs_error ~ CP_subset*Task_item.c + age.c + (1|SID), 
                           data = subset(error.model.df, Task == "Orthogonal"))

#compare 
anova(orth.overall.error.base, orth.overall.error.kl, orth.overall.error.int, test = 'LRT') #Only main effect of knower-level here, no interaction with numerosity
summary(orth.overall.error.kl)
```

##Error: Does error differ as a function of condition?

```{r}
#base model
overall.error.cond.base <- lmer(abs_error ~ CP_subset + Task_item.c + Task + age.c + (1|SID), 
                           data = error.model.df)

#add kl
overall.error.cond.kl <- lmer(abs_error ~ Condition + CP_subset + Task_item.c + Task + age.c + (1|SID), 
                           data = error.model.df)

#add interaction
overall.error.cond.int <- lmer(abs_error ~ Condition*CP_subset+ Task_item.c + Task + age.c + (1|SID), 
                           data = error.model.df)
#add 3-way interaction
overall.error.cond.3int <- lmer(abs_error ~ Condition*CP_subset*Task_item.c + Task + age.c + (1|SID), 
                           data = error.model.df)

#compare 
anova(overall.error.cond.base, overall.error.cond.kl, overall.error.cond.int, overall.error.cond.3int, test = 'LRT') #subset-knowers have higher absolute error for greater set sizes; significantly lower error on parallel task

summary(overall.error.cond.int)
```

##Error: Do CP-knowers have lower error on the Non-identical vs. identical conditions?

```{r}
#base model
cp.error.cond.base <- lmer(abs_error ~ Task_item.c + Task + age.c + (1|SID), 
                           data = subset(error.model.df, CP_subset == "CP"))

#add condition
cp.error.cond.kl <- lmer(abs_error ~ Condition + Task_item.c + Task + age.c + (1|SID), 
                           data = subset(error.model.df, CP_subset == "CP"))

#add interaction
cp.error.cond.int <- lmer(abs_error ~ Condition*Task_item.c + Task + age.c + (1|SID), 
                           data = subset(error.model.df, CP_subset == "CP"))

#compare 
anova(cp.error.cond.base, cp.error.cond.kl, cp.error.cond.int, test = 'LRT') #main effect of condition

summary(cp.error.cond.kl)
```

##Error: Do SS-knowers have lower error on the Non-identical vs. identical conditions?

```{r}
#base model
ss.error.cond.base <- lmer(abs_error ~ Task_item.c + Task + age.c + (1|SID), 
                           data = subset(error.model.df, CP_subset == "Subset"))

#add condition
ss.error.cond.kl <- lmer(abs_error ~ Condition + Task_item.c + Task + age.c + (1|SID), 
                           data = subset(error.model.df, CP_subset == "Subset"))

#add interaction
ss.error.cond.int <- lmer(abs_error ~ Condition*Task_item.c + Task + age.c + (1|SID), 
                           data = subset(error.model.df, CP_subset == "Subset"))

#compare 
anova(ss.error.cond.base, ss.error.cond.kl, ss.error.cond.int, test = 'LRT') #marginal effect of condition

summary(ss.error.cond.kl)
```

##Counting proficiency
Planned analysis with subset-knowers; no significant effect of counting proficiency
```{r}
#filter out the kids who don't have counting proficiency for now
model.df %<>%
  filter(count_proficiency != 5, 
         !is.na(count_proficiency))

#make base model
count.base <- glmer(Correct ~ Task_item.c + Task + age.c + (1|SID), 
                    family = "binomial", data = subset(model.df, CP_subset == "Subset"))

#add counting proficiency
count.prof <- glmer(Correct ~ count_proficiency.c + Task_item.c + Task + age.c + (1|SID), 
                    family = "binomial", data = subset(model.df, CP_subset == "Subset"))

#add interaction
count.int <- glmer(Correct ~ count_proficiency.c*Task_item.c + Task + age.c + (1|SID), 
                    family = "binomial", data = subset(model.df, CP_subset == "Subset"))

#compare
anova(count.base, count.prof, count.int, test = 'LRT')
```

What about error?
Subset-knowers
```{r}
#filter out the kids who don't have counting proficiency for now
error.model.df %<>%
  filter(count_proficiency != 5, 
         !is.na(count_proficiency))

#make base model
error.count.base <- lmer(abs_error ~ Task_item.c + Task + age.c + (1|SID), data = subset(error.model.df, CP_subset == "Subset"))

#add counting proficiency
error.count.prof <- lmer(abs_error ~ count_proficiency.c + Task_item.c + Task + age.c + (1|SID), data = subset(error.model.df, CP_subset == "Subset"))

#add interaction
error.count.int <- lmer(abs_error ~ count_proficiency.c*Task_item.c + Task + age.c + (1|SID), data = subset(error.model.df, CP_subset == "Subset"))

#compare
anova(error.count.base, error.count.prof, error.count.int, test = 'LRT')
summary(error.count.prof)
```

##Exploratory analysis with CP-knowers
No significant effect of counting proficiency here
```{r}
#filter out the kids who don't have counting proficiency for now
model.df %<>%
  filter(count_proficiency != 5, 
         !is.na(count_proficiency))

#make base model
cp.count.base <- glmer(Correct ~ Task_item.c + Task + age.c + (1|SID), 
                    family = "binomial", data = subset(model.df, CP_subset == "CP"))

#add counting proficiency
cp.count.prof <- glmer(Correct ~ count_proficiency.c + Task_item.c + Task + age.c + (1|SID), 
                    family = "binomial", data = subset(model.df, CP_subset == "CP"))

#add interaction
cp.count.int <- glmer(Correct ~ count_proficiency.c*Task_item.c + Task + age.c + (1|SID), 
                    family = "binomial", data = subset(model.df, CP_subset == "CP"))

#compare
anova(cp.count.base, cp.count.prof, cp.count.int, test = 'LRT')
```

Now CP-knowers
```{r}

#make base model
cp.error.count.base <- lmer(abs_error ~ Task_item.c + Task + age.c + (1|SID), data = subset(error.model.df, CP_subset == "CP"))

#add counting proficiency
cp.error.count.prof <- lmer(abs_error ~ count_proficiency.c + Task_item.c + Task + age.c + (1|SID), data = subset(error.model.df, CP_subset == "CP"))

#add interaction
cp.error.count.int <- lmer(abs_error ~ count_proficiency.c*Task_item.c + Task + age.c + (1|SID), data = subset(error.model.df, CP_subset == "CP"))

#compare
anova(cp.error.count.base, cp.error.count.prof, cp.error.count.int, test = 'LRT')
summary(cp.error.count.prof)
```

##Predicting performance by Highest Count
###Does highest count predict performance for CP- and subset-knowers?
```{r}
##remove highest count NAs
hc.df <- model.df %>%
  filter(!is.na(highest_count))

##cp
#make base model
hc.cp.base <- glmer(Correct ~ Task_item.c + Task + age.c + (1|SID), 
                    family = "binomial", data = subset(hc.df, CP_subset == "CP"))
#add hc
hp.cp.with.hc <- glmer(Correct ~ highest_count.c + Task_item.c + Task + age.c + (1|SID), 
                       family = "binomial", data = subset(hc.df, CP_subset == "CP"))

##compare
anova(hc.cp.base, hp.cp.with.hc, test = 'LRT') ##Not significant

##subset
#make base model
hc.ss.base <- glmer(Correct ~ Task_item.c + Task + age.c + (1|SID), 
                    family = "binomial", data = subset(hc.df, CP_subset == "Subset"))
#add hc
hp.ss.with.hc <- glmer(Correct ~ highest_count.c + Task_item.c + Task + age.c + (1|SID), 
                       family = "binomial", data = subset(hc.df, CP_subset == "Subset"))

##compare
anova(hc.ss.base, hp.ss.with.hc, test = 'LRT') ##Not significant
```


##Predicting performance by counting attempt
###Note that this is collapsing across counting and number language
Interesting - there's an interaction here, such that for CP-knowers who count on larger number trials, they are less accurate.
```{r}
counting.df <- model.df %>%
  filter(Task == "Parallel" | Task == "Orthogonal")%>%
  droplevels()

#cp-knowers
counting.attempt.cp.base <- glmer(Correct ~ Task_item.c + Task + age.c + (1|SID), 
                                  family = "binomial", data = subset(counting.df, CP_subset == "CP"))
#add counting attempts
counting.attempt.cp.count <- glmer(Correct ~ Counting.Number.language + Task_item.c + Task + age.c + (1|SID), 
                                  family = "binomial", data = subset(counting.df, CP_subset == "CP"))
#add interaction
counting.attempt.cp.int <- glmer(Correct ~ Counting.Number.language*Task_item.c + Task + age.c + (1|SID), 
                                  family = "binomial", data = subset(counting.df, CP_subset == "CP"))

#compare 
anova(counting.attempt.cp.base, counting.attempt.cp.count, counting.attempt.cp.int, test = 'LRT')
summary(counting.attempt.cp.int)


###subset-knowers
counting.attempt.ss.base <- glmer(Correct ~ Task_item.c + Task + age.c + (1|SID), 
                                  family = "binomial", data = subset(counting.df, CP_subset == "Subset"))
#add counting attempts
counting.attempt.ss.count <- glmer(Correct ~ Counting.Number.language + Task_item.c + Task + age.c + (1|SID), 
                                  family = "binomial", data = subset(counting.df, CP_subset == "Subset"))
#add interaction
counting.attempt.ss.int <- glmer(Correct ~ Counting.Number.language*Task_item.c + Task + age.c + (1|SID), 
                                  family = "binomial", data = subset(counting.df, CP_subset == "Subset"))

#compare 
anova(counting.attempt.ss.base, counting.attempt.ss.count, counting.attempt.ss.int, test = 'LRT') ##no significant effect of counting attempts/number language in ss-knowers
```

##Follow-up: Excluding trials where kids have counted
###Accuracy
Does this change anything in terms of CP-advantage?
```{r}
#create a base model that includes numerosity and task
overall.acc.base.no.count <- glmer(Correct ~ Task_item.c + Task + age.c + (1|SID), 
                          family = "binomial", data = subset(model.df, Counting.Number.language == "0"),
                          control=glmerControl(optimizer="bobyqa",
                          optCtrl=list(maxfun=2e4)))

#add CP_subset-knower status
overall.acc.kl.no.count <- glmer(Correct ~ CP_subset + Task_item.c + Task + age.c + (1|SID), 
                          family = "binomial", data = subset(model.df, Counting.Number.language == "0"), 
                          control=glmerControl(optimizer="bobyqa",
                          optCtrl=list(maxfun=2e4)))

#now add interaction
overall.acc.kl.int.no.count <- glmer(Correct ~ CP_subset*Task_item.c + Task + age.c + (1|SID), 
                          family = "binomial", data= subset(model.df, Counting.Number.language == "0"), 
                          control=glmerControl(optimizer="bobyqa",
                          optCtrl=list(maxfun=2e4)))

#compare
anova(overall.acc.base.no.count, overall.acc.kl.no.count, overall.acc.kl.int.no.count, test = 'LRT') 

summary(overall.acc.kl.no.count)
```

###Error
No, results are pretty much the same
```{r}
#base model
overall.error.base.nocount <- lmer(abs_error ~ Task_item.c + Task + age.c + (1|SID), 
                           data = subset(error.model.df, Counting.Number.language == "0"))

#add kl
overall.error.kl.no.count <- lmer(abs_error ~ CP_subset + Task_item.c + Task + age.c + (1|SID), 
                           data = subset(error.model.df, Counting.Number.language == "0"))

#add interaction
overall.error.int.no.count <- lmer(abs_error ~ CP_subset*Task_item.c + Task + age.c + (1|SID), 
                           data = subset(error.model.df, Counting.Number.language == "0"))

#compare 
anova(overall.error.base.nocount, overall.error.kl.no.count, overall.error.int.no.count, test = 'LRT') #marginal better performance for CP-knowers; significantly lower error on larger numerosities
summary(overall.error.int.no.count)
```

##Histogram of error
```{r}
error.df %>%
  ggplot(aes(x = abs_error, fill = CP_subset)) + 
  geom_histogram() + 
  facet_grid(Task ~ as.factor(Task_item), scale = "free_x") + 
  scale_x_continuous(breaks = seq(0, 10, 1)) + 
  theme(legend.position = "bottom")

all.data %>%
  filter(Task == "Parallel" | Task == "Orthogonal", 
         Trial_number != "Training")%>%
  mutate(Task_item = factor(Task_item)) %>%
  mutate(Response = as.numeric(as.character(Response)))%>%
  group_by(CP_subset, Task_item, Response, Task)%>%
  summarise(n = n())%>%
  mutate(n = ifelse(is.na(n), 0, as.numeric(n)))%>%
  ggplot(aes(x = Task_item, y = Response, fill = CP_subset, alpha = n)) +
  geom_tile() +
  # geom_text(aes(label = as.character(round(prop, 2))), 
  #           size = 2.5) +
  coord_fixed(ratio = .25)+
  scale_fill_manual(values = c(CP = "#1B9E77", Subset = "#D95F02")) + 
  theme_bw(base_size = 13) + 
  labs(x = "Set size", y = "Response") + 
  scale_y_continuous(breaks = seq(1, 15, 1)) + 
  xlim("3", "4", "6", "8", "10") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), 
        legend.position = "right", 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), 
        legend.title = element_text(size = 11)) +
  facet_grid(rows = vars(CP_subset), cols = vars(factor(Task, levels = c("Parallel", "Orthogonal"))))
ggsave("/Users/roseschneider/Documents/Projects/one-one/Analysis/Figures/corr_error_all.png", width = 7, height = 4)

#By condition, parallel
all.data %>%
  filter(Task == "Parallel",
         Trial_number != "Training")%>%
  mutate(Task_item = factor(Task_item)) %>%
  mutate(Response = as.numeric(as.character(Response)))%>%
  group_by(CP_subset, Task_item, Response, Condition)%>%
  summarise(n = n())%>%
  mutate(n = ifelse(is.na(n), 0, as.numeric(n)))%>%
  ggplot(aes(x = Task_item, y = Response, fill = CP_subset, alpha = n)) +
  geom_tile() +
  # geom_text(aes(label = as.character(round(prop, 2))), 
  #           size = 2.5) +
  coord_fixed(ratio = .25)+
  scale_fill_manual(values = c(CP = "#1B9E77", Subset = "#D95F02")) + 
  theme_bw(base_size = 13) + 
  labs(x = "Set size", y = "Response") + 
  scale_y_continuous(breaks = seq(1, 15, 1)) + 
  xlim("3", "4", "6", "8", "10") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), 
        legend.position = "right", 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), 
        legend.title = element_text(size = 11)) +
  facet_grid(rows = vars(CP_subset), cols = vars(Condition))
ggsave("/Users/roseschneider/Documents/Projects/one-one/Analysis/Figures/corr_error_parallel.png", width = 7, height = 4)

all.data %>%
  filter(Task == "Orthogonal",
         Trial_number != "Training")%>%
  mutate(Task_item = factor(Task_item)) %>%
  mutate(Response = as.numeric(as.character(Response)))%>%
  group_by(CP_subset, Task_item, Response, Condition)%>%
  summarise(n = n())%>%
  mutate(n = ifelse(is.na(n), 0, as.numeric(n)))%>%
  ggplot(aes(x = Task_item, y = Response, fill = CP_subset, alpha = n)) +
  geom_tile() +
  # geom_text(aes(label = as.character(round(prop, 2))), 
  #           size = 2.5) +
  coord_fixed(ratio = .25)+
  scale_fill_manual(values = c(CP = "#1B9E77", Subset = "#D95F02")) + 
  theme_bw(base_size = 13) + 
  labs(x = "Set size", y = "Response") + 
  scale_y_continuous(breaks = seq(1, 15, 1)) + 
  xlim("3", "4", "6", "8", "10") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), 
        legend.position = "right", 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), 
        legend.title = element_text(size = 11)) +
  facet_grid(rows = vars(CP_subset), cols = vars(Condition))
ggsave("/Users/roseschneider/Documents/Projects/one-one/Analysis/Figures/corr_error_orthogonal.png", width = 7, height = 4)

```

##Compute COV
```{r}
#for each participant
#hardcode n as 5, no participant has more or less
cov.df <- all.data %>%
  filter(Task == "Parallel" | 
           Task == "Orthogonal", 
         Trial_number != "Training")%>%
  dplyr::select(SID, Age, CP_subset, Task, Task_item, Response, Numerosity)%>%
  mutate(Task_item = as.numeric(as.character(Task_item)), 
         Response = as.numeric(as.character(Response)))%>%
  mutate(cov = sqrt((Task_item - Response)^2)/Task_item)

#where my outliers at
cov.outliers <- cov.df %>%
  filter(cov > .4)

##group by outliers
cov.df %<>%
  mutate(outlier = ifelse(cov > .25, 1, 0))

#are kids systematically outliers?
tmp <- cov.df %>%
  group_by(SID, CP_subset, Task)%>%
  summarise(n = sum(outlier))%>%
  filter(n > 2)

#correlation for CP across tasks
cor.test(subset(cov.df, CP_subset == "CP" & Task == "Parallel")$cov, 
         subset(cov.df, CP_subset == "CP" & Task == "Orthogonal")$cov)

#correlation for subset across tasks
cor.test(subset(cov.df, CP_subset == "Subset" & Task == "Parallel")$cov, 
         subset(cov.df, CP_subset == "Subset" & Task == "Orthogonal")$cov)

ggplot(cov.df, aes(x = Task_item, y = cov, color = CP_subset, group = CP_subset)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  facet_grid(~Task)

cov.df %>%
  group_by(Task, Task_item, CP_subset)%>%
  summarise(mean_COV = mean(cov, na.rm = TRUE)) %>% #not sure why there are NAs right now - fix
  ggplot(aes(x = Task_item, y = mean_COV, color = CP_subset, group = CP_subset)) + 
  geom_point() + 
  geom_line() + 
  facet_grid(~Task)
```


##Trying to analyze distribution of responses
```{r}
#get absolute error
error <- all.data %>%
  filter(Task == "Parallel" | 
           Task == "Orthogonal", 
         Trial_number != "Training") %>%
  mutate(error = (as.numeric(as.character(Task_item)) - as.numeric(as.character(Response))))

#plot absolute error distribution - parallel
error %>%
  filter(Task == "Parallel")%>%
  mutate(Task_item = as.numeric(as.character(Task_item)), 
         Response = as.numeric(as.character(Task_item))) %>%
  group_by(CP_subset, Task_item, error)%>%
  ggplot(aes(x = error, fill = CP_subset)) + 
  geom_histogram() + 
  geom_vline(xintercept = 0, linetype = "dashed") +
  theme_bw()+
  theme(legend.position = "none") +
  scale_fill_brewer(palette = "Dark2") + 
  facet_grid(CP_subset~Task_item) + 
  labs(title = 'Error distribution for Parallel')

# error %>%
#   filter(Task == "Orthogonal")%>%
#   mutate(Task_item = as.numeric(as.character(Task_item)), 
#          Response = as.numeric(as.character(Task_item))) %>%
#   group_by(CP_subset, Task_item, error)%>%
#   ggplot(aes(x = error, fill = CP_subset)) + 
#   geom_histogram() + 
#   geom_vline(xintercept = 0, linetype = "dashed") +
#   theme_bw()+
#   theme(legend.position = "none") +
#   scale_fill_brewer(palette = "Dark2") + 
#   facet_grid(CP_subset~Task_item) + 
#   labs(title = 'Error distribution for Orthogonal')

#boxplot of error
error %>%
  filter(Task == "Parallel") %>%
  mutate(Task_item = factor(Task_item, levels = c("3", "4", "6", "8", "10")))%>%
  group_by(CP_subset, Task_item)%>%
  ggplot(aes(x = Task_item, y = error, fill = CP_subset)) + 
  geom_boxplot()+
  scale_fill_brewer(palette = "Dark2") + 
  labs(title = 'Error: Parallel')

#leveneTest
#3
car::leveneTest(error ~ CP_subset, data = subset(error, Task_item == "3" & 
                                                   Task == "Parallel"), center = "mean")

#4
car::leveneTest(error ~ CP_subset, data = subset(error, Task_item == "4" & 
                                                   Task == "Parallel"), center = "mean")

#8
car::leveneTest(error ~ CP_subset, data = subset(error, Task_item == "8" & 
                                                   Task == "Parallel"), center = "mean")

#10
car::leveneTest(error ~ CP_subset, data = subset(error, Task_item == "10" & 
                                                   Task == "Parallel"), center = "mean")

#linear regression 


lm(var(error) ~  Age, data = subset(error, Task == "Parallel"))

#what if we just compute the variance of the error for each group
error %<>%
  mutate(abs.error = abs(as.numeric(as.character(Task_item)) - as.numeric(as.character(Response))))

error %>%
  filter(Task == "Parallel") %>%
  mutate(Task_item = factor(Task_item, levels = c("3", "4", "6", "8", "10")))%>%
  group_by(CP_subset, Task_item)%>%
  ggplot(aes(x = Task_item, y = abs.error, fill = CP_subset)) + 
  geom_boxplot()+
  scale_fill_brewer(palette = "Dark2") + 
  labs(title = 'Error: Parallel')

error %>%
  filter(Task == "Parallel")%>%
  mutate(Task_item = as.numeric(as.character(Task_item)), 
         Response = as.numeric(as.character(Task_item))) %>%
  group_by(CP_subset, Task_item, error)%>%
  ggplot(aes(x = abs.error, fill = CP_subset)) + 
  geom_histogram() +
  geom_vline(xintercept = mean(abs.error), linetype = "dashed") +
  theme_bw()+
  theme(legend.position = "none") +
  scale_fill_brewer(palette = "Dark2") + 
  facet_grid(CP_subset~Task_item) + 
  labs(title = 'Absolute Error distribution for Parallel')

error %>%
  filter(Task == "Parallel") %>%
  mutate(Task_item = factor(Task_item, levels = c("3", "4", "6", "8", "10")))%>%
  group_by(CP_subset, Task_item)%>%
  ggplot(aes(x = Task_item, y = error, fill = CP_subset)) + 
  geom_boxplot()+
  scale_fill_brewer(palette = "Dark2") + 
  labs(title = 'Error: Parallel')

error %<>%
  mutate(error.c = as.vector(scale(error, center = TRUE, scale = FALSE)))

error %>%
  filter(Task == "Parallel") %>%
  mutate(Task_item = factor(Task_item, levels = c("3", "4", "6", "8", "10")))%>%
  group_by(CP_subset, Task_item)%>%
  ggplot(aes(x = Task_item, y = error.c, fill = CP_subset)) + 
  geom_boxplot()+
  scale_fill_brewer(palette = "Dark2") + 
  labs(title = 'Error: Parallel')


var.df <- error %>%
  group_by(CP_subset, Numerosity, Task, Task_item)%>%
  summarise(var.abs.error = var(abs.error))

summary(lm(var.abs.error ~ Numerosity*CP_subset, data = subset(var.df, Task == "Parallel")))

##Notes: 
#Levene test will compare whether two variances are different with departures from normal distribution: This will tell us that variance for distributions which are centered on the median differ for a particular Task item between CP and SS knowers
#Comparing variance? Is that the right measure of clumpiness?



```

